{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOX\n",
    "# This is the dimensions of the unit cell\n",
    "(like transformation matrix in vesta but the entries are actual lengths, not units 1, 2, 8, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'box-filename.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16636/3944428803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#this .txt has the filenames for the unit cell dimensions of each unit cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'box-filename.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'box-filename.txt'"
     ]
    }
   ],
   "source": [
    "#creating a dataframe from \"box-filename.txt\"\n",
    "#this .txt has the filenames for the unit cell dimensions of each unit cell\n",
    "#\n",
    "filenames = pd.read_csv('box-filename.txt',header=None)\n",
    "filenames.head()\n",
    "#\n",
    "#need to find out again what c11 and box 104 etc means\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning column 0 in filenames into a list\n",
    "#there is only one collumn here so it's already kinda a list but important for later manipulation of df\n",
    "#and also important if the df had more than one column\n",
    "#\n",
    "filenames = list(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty list called boxes\n",
    "#iterates through box numbers\n",
    "#turns 3x3 matrix for unit cell parameters into a 1x9 object\n",
    "boxes = []\n",
    "for name in filenames:\n",
    "    cell = np.loadtxt('box/' + str(name), delimiter=' ').reshape(1,9)\n",
    "    boxes.append(cell[0])\n",
    "    \n",
    "print(boxes[19])\n",
    "#as far as I'm concerned it should be [x, 0, 0,      0, y, 0,     0, 0, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this reads off \"box-filenames.txt\" and then removes the .csv tag (with [:-4]) to create indexes for a dataframe\n",
    "ind = [str(name[:-4])for name in filenames]\n",
    "ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3x3 matrix transformed into 1x9,\n",
    "#this keeps track of them\n",
    "col = ['xx','xy','xz','yx','yy','yz','zx','zy','zz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how long the df should be (where do these come from again??)\n",
    "77+320+362+158+210+316+237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally creating the dataframe\n",
    "df_box = pd.DataFrame(boxes,index=ind, columns=col)\n",
    "df_box\n",
    "#why are the xz and zx values not zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POT\n",
    "### This is the pseudopotentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the feature vectors are created based off 11 parameters mentioned in the pseudopotential\n",
    "pot = pd.read_csv('potcar_param.csv')\n",
    "pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose\n",
    "pot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes collumn 1,2,3,4,9,10 (why?)\n",
    "table = pot.drop([1,2,3,4,9,10],axis=0).T\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an array for every row in the table abobe\n",
    "pot_name= np.array(pot.iloc[:,0])\n",
    "pot_Fe= np.array(pot.iloc[:,1])\n",
    "pot_O= np.array(pot.iloc[:,2])\n",
    "pot_Ti= np.array(pot.iloc[:,3])\n",
    "pot_Bi= np.array(pot.iloc[:,4])\n",
    "\n",
    "pot_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pot_Bi),len(pot_Ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coooord test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, reading the first file which contains coords of all the atoms\n",
    "bx = np.loadtxt('coord/c1-coord1',skiprows=1)\n",
    "bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bx).to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as what was done with the unit cell, but this time with atom coordinates\n",
    "filenames = pd.read_csv('coord-filename.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just an example of what is going on\n",
    "filenames[0],filenames[320],filenames[682],filenames[840],filenames[1050],filenames[1127],filenames[1443]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again reshaping into one dimension as machine learning requires input to be 1 dimensional\n",
    "coords = []\n",
    "for name in filenames:\n",
    "    c = np.loadtxt('coord/' + str(name), delimiter=' ').reshape(1,348)\n",
    "    coords.append(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coord pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords[0][:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 24 atoms are bismuth, so making list of bismuth positions for first coords file (24*3=72)\n",
    "Bi_xyz = coords[0][:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pot that has been made previously\n",
    "pot_Bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiply each position by the potential to create an array for each single bismuth position value (the decoupling thing)\n",
    "Bi_coord_pot = []\n",
    "for i in Bi_xyz:\n",
    "    Bi_coord_pot.append(i*pot_Bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be same no of arrays as there are xyz values for bismuth position\n",
    "len(Bi_coord_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, everything needs to be in one dimension\n",
    "np.array(Bi_coord_pot).reshape(1,792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the example above, creating the decoupled arrays for all 1680 different configurations\n",
    "Bi_all = []\n",
    "for i in range(0,1680):\n",
    "    Bi_xyz = coords[i][:72]\n",
    "    Bi_coord_pot = []\n",
    "    for i in Bi_xyz:\n",
    "        Bi_coord_pot.append(i*pot_Bi)\n",
    "    Bi_all.append(np.array(Bi_coord_pot).reshape(792))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Bi_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the same for Ti, Fe below\n",
    "Ti_all = []\n",
    "for i in range(0,1680):\n",
    "    Ti_xyz = coords[i][72:108]\n",
    "    Ti_coord_pot = []\n",
    "    for i in Ti_xyz:\n",
    "        Ti_coord_pot.append(i*pot_Ti)\n",
    "    Ti_all.append(np.array(Ti_coord_pot).reshape(396))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ti_all[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fe_all = []\n",
    "for i in range(0,1680):\n",
    "    Fe_xyz = coords[i][324:348]\n",
    "    Fe_coord_pot = []\n",
    "    for i in Fe_xyz:\n",
    "        Fe_coord_pot.append(i*pot_Fe)\n",
    "    Fe_all.append(np.array(Fe_coord_pot).reshape(264))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Fe_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Bi = pd.DataFrame(Bi_all)\n",
    "df_Bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Ti_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Fe_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ti_Fe = pd.concat([pd.DataFrame(Ti_all),pd.DataFrame(Fe_all)], axis=1, ignore_index=True)\n",
    "Ti_Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with all the coordinates concated together\n",
    "Bi_Ti_Fe = pd.concat([pd.DataFrame(Bi_all),Ti_Fe], axis=1, ignore_index=True)\n",
    "Bi_Ti_Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the unit cell dimesions to the front of the coordinates df\n",
    "box_Bi_Ti_Fe = pd.concat([pd.DataFrame(boxes),Bi_Ti_Fe], axis=1, ignore_index=True)\n",
    "box_Bi_Ti_Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box_Ti_Fe = pd.concat([pd.DataFrame(boxes),Ti_Fe], axis=1, ignore_index=True)\n",
    "#box_Ti_Fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enegies for each configuration\n",
    "energy_df = pd.read_csv('oszicar/energys.csv',header=None, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the energies to position dataframe (at the end as it is the target)\n",
    "df = pd.concat([box_Bi_Ti_Fe,energy_df[1]], axis=1, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "#### clustering all the different configurations together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km(n_clusters=7,random_state=1).fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = PCA(n_components=2).fit_transform(km(n_clusters=7,random_state=1).fit_transform(df))\n",
    "xy[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(0.02*xy[:,0],0.02*xy[:,1], '.')#, label = labels)\n",
    "#plt.xlim(-80, -75)\n",
    "#plt.legend(plot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I believe that this implies there are 7 distinct phases/structures/somethinglikethat for all 1680 configurations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1680 from 1680 configs and 1462 from atom positions and energies?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_test split, deciding how big each category should be\n",
    "valid_samples = int(round(.2*df.shape[0]))\n",
    "test_samples = int(round(.1*df.shape[0]))\n",
    "train_samples = df.shape[0]-test_samples-valid_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, valid_samples, test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed\n",
    "RNG_SEED = 40\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "#creating test/train/val split based on sizes above\n",
    "#randomly add to val, then remove all from all_f if in val\n",
    "#then randomly add to test from what is left, everything in test removed from all_f\n",
    "#all thats left is testing data\n",
    "#note this is just the indexes, being randomly assigned, can then go back to df using indices and create actual dfs\n",
    "all_f = df.index.tolist().copy()\n",
    "valid_data = np.random.choice(all_f,\n",
    "                              size=valid_samples,\n",
    "                              replace=False)\n",
    "\n",
    "\n",
    "all_f = [ f for f in all_f if f not in valid_data]\n",
    "test_data = np.random.choice(all_f,\n",
    "                              size=test_samples,\n",
    "                              replace=False)\n",
    "\n",
    "all_f = [ f for f in all_f if f not in test_data]\n",
    "train_data = all_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing that some entries are missing from train when compared to all (indexes don't match)\n",
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isin(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the train val test split and checking they are the correct size\n",
    "cp_train = df.loc[train_data]\n",
    "cp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_valid = df.loc[valid_data]\n",
    "cp_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_test = df.loc[test_data]\n",
    "cp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling the split\n",
    "cp_train_sample = cp_train.sample(n=1100, random_state=RNG_SEED)\n",
    "cp_valid_sample = cp_valid.sample(n=150, random_state=RNG_SEED)\n",
    "cp_test_sample = cp_test.sample(n=150, random_state=RNG_SEED)\n",
    "\n",
    "len(cp_train_sample),len(cp_valid_sample),len(cp_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_train_sample.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final variables with split made (no energy here)\n",
    "y_train = cp_train_sample.iloc[:,1461]\n",
    "y_valid = cp_valid_sample.iloc[:,1461]\n",
    "y_test = cp_test_sample.iloc[:,1461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling and normalising the data\n",
    "#.iloc[:,:-1] removes the energy values from the scaling\n",
    "#energy is the target\n",
    "X_train = scaler.fit_transform(cp_train_sample.iloc[:,:-1])\n",
    "X_valid = scaler.transform(cp_valid_sample.iloc[:,:-1])\n",
    "X_test  = scaler.transform(cp_test_sample.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "### methods\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### metrics\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_methods = [DummyRegressor,\n",
    "                Ridge,\n",
    "                KNeighborsRegressor,\n",
    "                SVR,\n",
    "                LinearSVR,\n",
    "                AdaBoostRegressor,\n",
    "                GradientBoostingRegressor,\n",
    "                ExtraTreesRegressor,\n",
    "                RandomForestRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_methods[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measurements\n",
    "\n",
    "def meas_method (act, pred):\n",
    "    r2   = r2_score(act, pred)\n",
    "    mae  = mean_absolute_error(act,pred)\n",
    "    rmse = mean_squared_error(act,pred,squared=False)\n",
    "    \n",
    "    return r2, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Evaluate\n",
    "#intialises method, starts a timer, trains the models, timer finishes, time taken to train found\n",
    "#accuracy on training validation set found\n",
    "\n",
    "def evaluate (X_train, y_train, X_valid, y_valid, methods):\n",
    "    \n",
    "    fit_time   = []\n",
    "    meas_train = []\n",
    "    meas_valid = []\n",
    "    pred_valid = []\n",
    "    for m in methods:\n",
    "    \n",
    "        method =m()\n",
    "        start = time()\n",
    "        method.fit(X_train, y_train)\n",
    "        end = time()\n",
    "        delta = end-start\n",
    "        print(m.__name__, '{:0.4f}'.format(delta)+'s')\n",
    "        y_pred_t = method.predict(X_train)\n",
    "        y_pred_v = method.predict(X_valid)\n",
    "        \n",
    "        fit_time.append(delta)\n",
    "        pred_valid.append(y_pred_v)\n",
    "        meas_train.append(meas_method(y_train,y_pred_t))\n",
    "        meas_valid.append(meas_method(y_valid,y_pred_v))\n",
    "    \n",
    "    return meas_train, meas_valid, pred_valid, fit_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = evaluate(X_train,y_train,X_valid, y_valid,nine_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define plot + regression y_pred\n",
    "#function to make a plot of pred vs actual energy for each model\n",
    "def plot_method (act, pred, method):\n",
    "    xy_max = np.max([np.max(act), np.max(pred)])\n",
    "    xy_min = np.min([np.min(act), np.min(pred)])\n",
    "    polyfit = np.polyfit(act,pred,deg=1)\n",
    "    reg_lsp = np.poly1d(polyfit)\n",
    "    \n",
    "    #print (reg_lsp, reg_lsp[1])\n",
    "    \n",
    "    line = [ i+.5 for i in range(int(xy_min)-1,int(xy_max)+1,2)]\n",
    "    reg_lsm = reg_lsp(line)\n",
    "    \n",
    "    plt.plot(act,pred, 'o', ms=9, mec= 'k', mfc= 'silver', alpha=.4)\n",
    "    plt.plot([xy_min,xy_max],[xy_min,xy_max],'k--', label='Target')\n",
    "    plt.plot(line,reg_lsm,'r-', label ='Linear fit')#+str(reg_lsp))   \n",
    "    \n",
    "    #plt.\n",
    "   # plt.xlim(xy_min-.2,xy_max+.2)\n",
    "    plt.xlabel(r'Actual Energy',fontsize=16)\n",
    "    plt.ylabel(r'Predicted Energy',fontsize=16)\n",
    "    plt.legend(loc='upper left',fontsize=14)\n",
    "    plt.title(str(method.__name__)+\n",
    "              ', score='+ '{:0.4f}'.format(r2_score(act,pred))\n",
    "              ,fontsize=20)\n",
    "    #plt.show()\n",
    "    return f'{reg_lsp[1]:0.6f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([np.max(y_valid),np.max(e[2][2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_five = plot_method (y_valid,e[2][2],nine_methods[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_train = e[0]\n",
    "meas_valid = e[1]\n",
    "time_fitti = e[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating mini datafames for all the different values\n",
    "#then putting them all together to see how the models perform\n",
    "df_time = pd.DataFrame(time_fitti, columns=['Fitting time'],\n",
    "                       index=[m.__name__ for m in nine_methods])\n",
    "\n",
    "df_train = pd.DataFrame(meas_train, \n",
    "                        columns=['r2_score_train', 'mae_train', 'rmse_train'],\n",
    "                        index=[m.__name__ for m in nine_methods])\n",
    "\n",
    "df_coef = pd.DataFrame(e_five, columns=['coef.lin_valid'],\n",
    "                       index=[m.__name__ for m in nine_methods])\n",
    "\n",
    "df_valid = pd.DataFrame(meas_valid, \n",
    "                   columns=['r2_score_valid', 'mae_valid', 'rmse_valid'], \n",
    "                   index=[m.__name__ for m in nine_methods])\n",
    "\n",
    "df = pd.concat([df_time,df_train,df_valid,df_coef], axis=1)\n",
    "df = df.sort_values('r2_score_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just making it look nicer\n",
    "def highlight_cols(x): \n",
    "      \n",
    "    # copy df to new - original data is not changed \n",
    "    df = x.copy() \n",
    "      \n",
    "    # select all values to green color \n",
    "    df.loc[:, :] = 'background-color: white and silver'\n",
    "      \n",
    "    # overwrite values grey color \n",
    "    df['r2_score_valid'] = 'background-color: lightgrey'\n",
    "      \n",
    "    # return color df \n",
    "    return df  \n",
    "  \n",
    "df_highlight = df.style.apply(highlight_cols, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.subplots_adjust(wspace=.3,hspace=.3)\n",
    "\n",
    "e_five =[]\n",
    "\n",
    "for j in range(0,9):\n",
    "    plt.subplot(3,3,j+1)\n",
    "    e_five.append(plot_method (y_valid,e[2][j],nine_methods[j]))\n",
    "    \n",
    "plt.savefig('nine_ML_methods.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplots_adjust(wspace=.3,hspace=.3)\n",
    "\n",
    "e_five =[]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "e_five.append(plot_method (y_valid,e[2][1],nine_methods[1]))\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "e_five.append(plot_method (y_valid,e[2][3],nine_methods[3]))\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "e_five.append(plot_method (y_valid,e[2][8],nine_methods[8]))\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "e_five.append(plot_method (y_valid,e[2][6],nine_methods[6]))\n",
    "    \n",
    "plt.savefig('four_ML_methods.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assingments for Orla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Add a new column with the label of the structure, for instance c11. And print with the plot with labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Train the model with configuration that are not in the validation set. For instance, Train with c1, c11, 12, c13, c14, c21, and use only c22 in validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Work with cross-validation. PLUS: fix the \"unbalacing\" problem. For instance, 50 samples from each configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_plot = []\n",
    "labels = [1, 11, 12, 13, 14, 21 ,22]\n",
    "for i in labels:\n",
    "    x = np.loadtxt('oszicar/energy'+str(i)+'.csv')\n",
    "    energy_plot.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [np.argsort(energy_plot[j][:,0]) for j in range(7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [] \n",
    "for j in range(7):\n",
    "    x.append([energy_plot[j][i,0] for i in order[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [] \n",
    "for j in range(7):\n",
    "    y.append([energy_plot[j][i,1] for i in order[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels = ['c1', 'c11', 'c12', 'c13', 'c14','c21','c22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = pd.read_csv(\"../../Bi6Ti5O18/ISAF-plots/BTFO-2_energy.csv\")\n",
    "m5.sort_values(by=['energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = m5.iloc[5,0]\n",
    "threshold = -770.120\n",
    "\n",
    "index = np.array(m5['config'])\n",
    "delta = ((np.array(m5['energy'])-min(np.array(m5['energy'])))*1000/4)\n",
    "delta1= ['+'+str(\"{:.2f}\".format(i)) for i in delta]\n",
    "c = np.array(['config. '+str(i) for i in np.array(m5['config'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(m5['energy'])-min(np.array(m5['energy'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.figsize=(6,6)\n",
    "plt.subplots_adjust(wspace=.5,hspace=.5)\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot(c,\n",
    "         (np.array(m5['energy'])-min(np.array(m5['energy'])))*1000/4,\n",
    "         'o')#, c=\"green\")\n",
    "\n",
    "plt.vlines(.5,.0,300,color=\"black\",alpha=.7,linestyles='dashed')\n",
    "plt.vlines(4.5,0,300,color=\"black\",alpha=.7,linestyles='dashed')\n",
    "#plt.hlines((threshold-min(np.array(m5['energy']))),0.0,10)\n",
    "\n",
    "#plt.tick_params(\n",
    "#    axis='y',          # changes apply to the x-axis\n",
    "#    which='both', right = True,     # both major and minor ticks are affected\n",
    " #  # bottom=False,      # ticks along the bottom edge are off\n",
    "    #top=False,         # ticks along the top edge are off\n",
    " #   labelright=True)\n",
    "\n",
    "#plt.ylim(-5,120)\n",
    "#plt.tick_params(axis ='both')#,labelright=True)\n",
    "plt.yticks(np.arange(0.0,310, step=50),size=14)\n",
    "plt.ylabel(\"$\\Delta$ E  (meV / f.u.)\" ,size=16 )\n",
    "plt.xticks(rotation=45,size=14)\n",
    "plt.xlabel(\"\",size=16)\n",
    "plt.title('Bi$_6$Ti$_3$Fe$_{2}$O$_{18}$ Cation Distributions',size=16)\n",
    "\n",
    "#plt.title(\"LDA+U functional\",size=16)\n",
    "#plt.savefig('../1-year-presentation/plots/m5-ldau.png',dpi=100,edgecolor='none')\n",
    "plt.savefig('B6TFO.png', dpi=600, edgecolor='none', bbox_inches = 'tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.subplots_adjust(wspace=.3,hspace=.3)\n",
    "\n",
    "for i in range(7):\n",
    "    n = 7-i-1\n",
    "    plt.plot(x[n], y[n], '-', linewidth=2,\n",
    "             markersize=20,label=labels)\n",
    "\n",
    "plt.vlines(100,-770.5,-767,color=\"black\",alpha=.7,linestyles='dashed')\n",
    "plt.ylim(-770.5,-767.5)\n",
    "plt.xlim(-10,260)\n",
    "plt.legend(plot_labels[::-1],fontsize=14)\n",
    "plt.ylabel(\" Energy ( eV ) \" ,size=16 )\n",
    "plt.xlabel(\"Steps\",size=16)\n",
    "plt.title('Bi$_6$Ti$_3$Fe$_{2}$O$_{18}$ Cation Distributions',size=16)\n",
    "plt.savefig('B6TFO-energy-steps.png', dpi=600, edgecolor='none', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
